{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0738778f",
   "metadata": {},
   "source": [
    "This is the reference notebook. All code has been (will be)refactored into src/ and tested via tests/. See scripts/train.py for the automated pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eee3047",
   "metadata": {},
   "source": [
    "# From Scratch: Building a Neural Network with NumPy\n",
    "\n",
    "This notebook contains an end-to-end implementation of a simple feedforward neural network using **only NumPy** â€” no deep learning frameworks involved. It aims to demystify the inner workings of neural networks by walking through each component step-by-step, with a strong focus on **clarity, interactivity, and visualization**.\n",
    "\n",
    "## Key Features\n",
    "\n",
    "- **Manual forward and backward passes** (no autograd)\n",
    "\n",
    "> *This project is inspired by university coursework, but developed independently from scratch to reinforce my understanding and extend the ideas further.*\n",
    "\n",
    "Author: Elena Arseneva (ea.arseneva@gmail.com)\n",
    "\n",
    "Last modified: June 4, 2025\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b1afb4b",
   "metadata": {},
   "source": [
    "## Plans:\n",
    "### Data and Training\n",
    "- (DONE) ~~Early stopping mechanism to prevent overfitting~~    (4)\n",
    "- (DONE) ~~Implement k-fold cross-validation~~                  (6)\n",
    "- (DONE) ~~Learning rate scheduling~~                           (5)\n",
    "- Training on other toy datasets (e.g., digit recognition or synthetic classification)                                      (11)\n",
    "\n",
    "### Visualization and Interactivity\n",
    "- (DONE) ~~Decision boundary visualization during training~~    (2)\n",
    "- (DONE) ~~Loss and accuracy plots after training~~  (1)\n",
    "- (DONE) ~~Visual explanation of gradients and weight updates~~ (3)\n",
    "\n",
    "\n",
    "### Model Improvements\n",
    "- Batch normalization implementation                 (9)\n",
    "- Dropout layers for regularization                  (10)\n",
    "- (DONE) ~~Different weight initialization strategies~~         (8) \n",
    "- Additional optimizers (Adam, RMSprop)              (7)\n",
    "\n",
    "### Future Improvements (OPTIONAL)\n",
    "- Training history tracking and logging\n",
    "- Model checkpointing and saving\n",
    "- Comprehensive testing suite\n",
    "- Performance optimization\n",
    "- Interactive sliders for hyperparameters (learning rate, network architecture)                                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970a1e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import time\n",
    "import copy\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import List, Tuple, Optional, Union, Dict, Any\n",
    "\n",
    "# Third-party imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpecFromSubplotSpec \n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59c5524",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility functions\n",
    "\n",
    "def softmax(x: np.ndarray) -> np.ndarray:\n",
    "    '''\n",
    "    Compute softmax values for each sets of scores in x.\n",
    "    x: 2D array of shape (n_samples, n_classes)\n",
    "    return: 2D array of shape (n_samples, n_classes) with softmax probabilities\n",
    "    \n",
    "    The softmax function is defined as:\n",
    "    softmax(x_i) = exp(x_i) / sum(exp(x_j))\n",
    "    where x_i is the i-th element of the input vector x and the sum is over all elements in x.\n",
    "    '''\n",
    "    # Subtract max for numerical stability\n",
    "    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "\n",
    "def generate_spiral_data(n_points_per_class: int, n_classes: int) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    '''\n",
    "    Generate spiral data for classification.\n",
    "    n_points_per_class: number of points per class\n",
    "    n_classes: number of classes\n",
    "    return: tuple (X, y_one_hot)\n",
    "    X: 2D array of shape (n_samples, 2) with the data points\n",
    "    y_one_hot: 2D array of shape (n_samples, n_classes) with one-hot encoded labels\n",
    "    '''\n",
    "    x = []\n",
    "    y = []\n",
    "    for j in range(n_classes):\n",
    "        ix = range(n_points_per_class * j, n_points_per_class * (j + 1))\n",
    "        r = np.linspace(0.0, 1, n_points_per_class)\n",
    "        t = np.linspace(j * 4, (j + 1) * 4, n_points_per_class) + np.random.randn(n_points_per_class) * 0.2\n",
    "        x1 = r * np.sin(t)\n",
    "        x2 = r * np.cos(t)\n",
    "        x.append(np.c_[x1, x2])\n",
    "        y.append(np.full(n_points_per_class, j))\n",
    "    x = np.vstack(x)\n",
    "    y = np.hstack(y)\n",
    "    y_one_hot = np.eye(n_classes)[y]\n",
    "    return x, y_one_hot\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89cde36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define abstract classes for Layer, Loss, Optimizer\n",
    "\n",
    "class Layer(ABC):\n",
    "    '''\n",
    "    Abstract base class for all layers in the neural network.\n",
    "    Each layer should implement the forward and backward methods, \n",
    "    the instances store their input and output dimensions.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, input_dim: Optional[int] = None, output_dim: Optional[int] = None) -> None:\n",
    "        self._input_dim = input_dim\n",
    "        self._output_dim = output_dim\n",
    "        self.input = None\n",
    "\n",
    "    @property\n",
    "    def input_dim(self) -> Optional[int]:\n",
    "        return self._input_dim\n",
    "\n",
    "    @property\n",
    "    def output_dim(self) -> Optional[int]:\n",
    "        return self._output_dim\n",
    "    \n",
    "    \n",
    "    @abstractmethod\n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Forward pass through the layer.\n",
    "        Args:\n",
    "            x (np.ndarray): Input array of shape (batch_size, input_dim).\n",
    "        Returns:\n",
    "            np.ndarray: Output array of shape (batch_size, output_dim).\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def backward(self, grad: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Backward pass through the layer.\n",
    "        Args:\n",
    "            grad (np.ndarray): Gradient of the loss with respect to the output. \n",
    "            The shape should be (batch_size, output_dim).\n",
    "        Returns:\n",
    "            np.ndarray: Gradient of the loss with respect to the input.\n",
    "            The shape should be (batch_size, input_dim).\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "class Loss(ABC):\n",
    "    '''\n",
    "    Abstract base class for all loss functions.\n",
    "    Each loss function should implement the forward and backward methods.\n",
    "    '''\n",
    "    \n",
    "    @abstractmethod\n",
    "    def forward(self, y_true: np.ndarray, y_pred: np.ndarray) -> Tuple[float, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Forward pass through the loss function.\n",
    "        Args:\n",
    "            y_true (np.ndarray): True labels. The shape should be (batch_size,).\n",
    "            y_pred (np.ndarray): Predicted labels. The shape should be (batch_size,).\n",
    "        Returns:\n",
    "            tuple: A tuple containing the loss value and the predicted probabilities.\n",
    "            The first element is a scalar (loss value), and the second element is an array of shape (batch_size,).\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def backward(self, y_true: np.ndarray, y_pred: np.ndarray, probs: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Backward pass through the loss function.\n",
    "        Args:\n",
    "            y_true (np.ndarray): True labels. The shape should be (batch_size,).\n",
    "            y_pred (np.ndarray): Predicted labels. The shape should be (batch_size,).\n",
    "            probs (np.ndarray): Predicted probabilities. The shape should be (batch_size,).\n",
    "        Returns:\n",
    "            np.ndarray: Gradient of the loss with respect to the predictions. The shape should be (batch_size,).\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "class Optimizer(ABC):\n",
    "    @abstractmethod\n",
    "    def step(self, params: np.ndarray, grads: np.ndarray) -> None:\n",
    "        \"\"\"\n",
    "        Update the parameters based on the gradients.\n",
    "        Args:\n",
    "            params (np.ndarray): Parameters to be updated. The shape should be (num_params,).\n",
    "            grads (np.ndarray): Gradients of the loss with respect to the parameters. The shape should be (num_params,).\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def update_learning_rate(self, new_lr: float) -> None:\n",
    "        \"\"\"\n",
    "        Update the learning rate of the optimizer.\n",
    "        Args:\n",
    "            new_lr (float): New learning rate.\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        self.learning_rate = new_lr\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7162cb0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define concrete implementation of Layer: Linear, ReLU, and Sequential\n",
    "\n",
    "class ReLU(Layer):\n",
    "    '''\n",
    "    ReLU layer.\n",
    "    Applies the ReLU activation function element-wise to the input.\n",
    "    The ReLU function is defined as f(x) = max(0, x).\n",
    "    '''\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        self.input = x\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    def backward(self, grad: np.ndarray) -> np.ndarray:\n",
    "        assert grad.shape == self.input.shape, f\"Gradient shape {grad.shape} does not match input shape {self.input.shape}\"\n",
    "        # Gradient of ReLU is 1 for positive inputs, 0 for negative inputs\n",
    "        return grad * (self.input > 0)\n",
    "    \n",
    "    \n",
    "class Linear(Layer):\n",
    "    '''\n",
    "    Linear layer.\n",
    "    Applies a linear transformation to the input data.\n",
    "    The transformation is defined as y = xW + b, where W is the weight matrix and b is the bias vector.\n",
    "    '''\n",
    "    def __init__(self, input_dim: int, output_dim: int) -> None:\n",
    "        assert input_dim > 0 and output_dim > 0, \"Input and output dimensions of a Linear layer must be positive integers.\"\n",
    "        super().__init__(input_dim, output_dim)\n",
    "        # Initialize weights and bias\n",
    "        #self.weights = np.random.randn(input_dim, output_dim) * 0.01\n",
    "        # Xavier/Glorot initialization instead of fixed scale\n",
    "        self.weights = np.random.randn(input_dim, output_dim) * np.sqrt(2.0 / input_dim)\n",
    "        self.bias = np.zeros((1, output_dim))\n",
    "        self.grad_weights = None\n",
    "        self.grad_bias = None   \n",
    "\n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        assert x.shape[1] == self.weights.shape[0], f\"Input shape {x.shape} does not match expected shape (batch_size, {self.weights.shape[0]})\"\n",
    "        assert self.weights.shape[1] == self.bias.shape[1], f\"Weights shape {self.weights.shape} does not match bias shape {self.bias.shape}\"\n",
    "        self.input = x\n",
    "        return x @ self.weights + self.bias\n",
    "\n",
    "    def backward(self, grad: np.ndarray) -> np.ndarray:\n",
    "        assert grad.shape[1] == self.bias.shape[1], f\"Gradient shape {grad.shape} does not match bias shape {self.bias.shape}\"\n",
    "        assert grad.shape[0] == self.input.shape[0], f\"Gradient shape {grad.shape} does not match input shape {self.input.shape}\"\n",
    "        # Gradient of the loss with respect to the input\n",
    "        grad_input = grad @ self.weights.T\n",
    "        # Gradient of the loss with respect to the weights and bias\n",
    "        self.grad_weights = self.input.T @ grad\n",
    "        self.grad_bias = np.sum(grad, axis=0, keepdims=True)\n",
    "              \n",
    "        # Monitor gradient statistics\n",
    "        grad_stats = {\n",
    "            'input_grad_max': np.abs(grad_input).max(),\n",
    "            'weight_grad_max': np.abs(self.grad_weights).max(),\n",
    "            'bias_grad_max': np.abs(self.grad_bias).max()\n",
    "        }\n",
    "    \n",
    "        if any(np.isnan(v) or np.isinf(v) for v in grad_stats.values()):\n",
    "            raise ValueError(f\"Invalid gradients detected: {grad_stats}\")\n",
    "        \n",
    "        return grad_input \n",
    "    \n",
    "class Sequential(Layer):\n",
    "    '''\n",
    "    Sequential model.\n",
    "    A container for stacking layers in a linear fashion.\n",
    "    The input to the first layer is the input to the model, and the output of the last layer is the output of the model.\n",
    "    '''\n",
    "    def __init__(self, layers: List[Layer]) -> None:\n",
    "        self.layers = layers\n",
    "        super().__init__(layers[0].input_dim, layers[-1].output_dim)\n",
    "        self.__check_consistency__()\n",
    "\n",
    "    def __check_consistency__(self) -> None:\n",
    "        assert len(self.layers) > 1, \"Sequential model must have at least one layer.\"\n",
    "        assert self.layers[0].input_dim is not None, \"First layer input dimension must be specified.\"\n",
    "        assert self.layers[-1].output_dim is not None, \"Last layer output dimension must be specified.\"\n",
    "        assert self.layers[0].input_dim == self.input_dim, f\"First layer input dimension {self.layers[0].input_dim} does not match expected input dimension {self.input_dim}\"\n",
    "        assert self.layers[-1].output_dim == self.output_dim, f\"Last layer output dimension {self.layers[-1].output_dim} does not match expected output dimension {self.output_dim}\"\n",
    "        current_dim = self.input_dim\n",
    "        mismatch_list = []\n",
    "        for layer in self.layers:\n",
    "            if layer.input_dim != None:\n",
    "                if layer.input_dim != current_dim: \n",
    "                    mismatch_list.append(f\"Layer {layer.__class__.__name__} input dimension {layer.input_dim} does not match expected input dimension {current_dim}\")\n",
    "                current_dim = layer.output_dim\n",
    "        assert len(mismatch_list) == 0, f\"Layer dimension mismatch: {'\\n'.join(mismatch_list)}\"\n",
    "                        \n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        assert x.shape[1] == self.layers[0].input_dim, f\"Input shape {x.shape} does not match expected shape (batch_size, {self.layers[0].input_dim})\"\n",
    "        for layer in self.layers:\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "\n",
    "    def backward(self, grad: np.ndarray) -> np.ndarray:\n",
    "        for layer in reversed(self.layers):\n",
    "            grad = layer.backward(grad)\n",
    "        return grad\n",
    "    \n",
    "    def summary(self) -> None:\n",
    "        \"\"\"Print a summary of the model architecture.\"\"\"\n",
    "        print(\"Model Summary:\")\n",
    "        print(\"-\" * 50)\n",
    "        total_params = 0\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            if isinstance(layer, Linear):\n",
    "                params = np.prod(layer.weights.shape) + np.prod(layer.bias.shape)\n",
    "                total_params += params\n",
    "                print(f\"Layer {i}: {layer.__class__.__name__}, \"\n",
    "                      f\"Input: {layer.input_dim}, Output: {layer.output_dim}, \"\n",
    "                      f\"Parameters: {params}\")\n",
    "            else:\n",
    "                print(f\"Layer {i}: {layer.__class__.__name__}\")\n",
    "        print(\"-\" * 50)\n",
    "        print(f\"Total parameters: {total_params}\")\n",
    "        print(\"-\" * 50)\n",
    "        print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb14196b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define concrete implementation of Loss: MeanSquaredError and CrossEntropy\n",
    "\n",
    "class MeanSquaredError(Loss):\n",
    "    '''\n",
    "    Mean Squared Error (MSE) loss function.\n",
    "    It measures the average squared difference between the predicted and true values.\n",
    "    The MSE is defined as:\n",
    "    MSE = (1/n) * sum((y_true - y_pred)^2)\n",
    "    where n is the number of samples, y_true is the true labels, and y_pred is the predicted labels.\n",
    "    '''\n",
    "    def forward(self, y_true: np.ndarray, y_pred: np.ndarray) -> tuple:\n",
    "        assert y_true.shape == y_pred.shape, f\"True labels shape {y_true.shape} does not match predicted labels shape {y_pred.shape}\"\n",
    "        return np.mean(np.square(y_true - y_pred)), y_pred\n",
    "\n",
    "    def backward(self, y_true: np.ndarray, y_pred: np.ndarray) -> np.ndarray:\n",
    "        assert y_true.shape == y_pred.shape, f\"True labels shape {y_true.shape} does not match predicted labels shape {y_pred.shape}\"\n",
    "        return 2 * (y_pred - y_true) / y_true.size\n",
    "    \n",
    "    \n",
    "class CrossEntropySoftMax(Loss):\n",
    "    '''\n",
    "    Cross-entropy loss function with softmax activation.\n",
    "    It is used for multi-class classification problems.\n",
    "    The cross-entropy loss is defined as:\n",
    "    CE = -sum(y_true * log(probs))\n",
    "    where y_true is the true labels (one-hot encoded) and probs is the predicted probabilities.\n",
    "    '''\n",
    "    \n",
    "    def forward(self, y_true: np.ndarray, y_pred_logits: np.ndarray) -> tuple:\n",
    "        '''\n",
    "        Forward pass through the cross-entropy loss function.\n",
    "        Args:\n",
    "            y_true (np.ndarray): True labels (one-hot encoded). The shape should be (batch_size, n_classes).\n",
    "            y_pred_logits (np.ndarray): Predicted logits. The shape should be (batch_size, n_classes).\n",
    "        Returns:\n",
    "            tuple: A tuple containing the loss value and the predicted probabilities.\n",
    "            The first element is a scalar (loss value), and the second element is an array of shape (batch_size, n_classes).\n",
    "        '''\n",
    "        assert y_true.shape == y_pred_logits.shape, f\"True labels shape {y_true.shape} does not match predicted logits shape {y_pred_logits.shape}\"\n",
    "        #apply softmax to the predictions\n",
    "        probs = softmax(y_pred_logits)\n",
    "        loss = -np.sum(y_true * np.log(probs + 1e-15)) / y_true.shape[0]\n",
    "        return loss, probs\n",
    "\n",
    "    def backward(self, y_true: np.ndarray, probs: np.ndarray) -> np.ndarray:\n",
    "        '''\n",
    "        Backward pass through the cross-entropy loss function.\n",
    "        Args:\n",
    "            y_true (np.ndarray): True labels (one-hot encoded). The shape should be (batch_size, n_classes).\n",
    "            probs (np.ndarray): Predicted probabilities. The shape should be (batch_size, n_classes).\n",
    "        Returns:\n",
    "            np.ndarray: Gradient of the loss with respect to the predictions. The shape should be (batch_size, n_classes).\n",
    "        '''\n",
    "        \n",
    "        assert y_true.shape == probs.shape, f\"True labels shape {y_true.shape} does not match prediction shape {probs.shape}\"\n",
    "        return (probs - y_true)/y_true.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cca352b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define concrete implementation of Optimizer: SGD\n",
    "\n",
    "class SGD(Optimizer):\n",
    "    '''\n",
    "    Stochastic Gradient Descent (SGD) optimizer.\n",
    "    It updates the parameters using the gradients and a learning rate.\n",
    "    The update rule is defined as:\n",
    "    params = params - learning_rate * grads\n",
    "    where params are the parameters to be updated, learning_rate is the learning rate, and grads are the gradients.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, learning_rate: float = 0.01) -> None:\n",
    "        assert learning_rate > 0, \"Learning rate must be a positive number.\"\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def step(self, params: np.ndarray, grads: np.ndarray) -> None:\n",
    "        assert params.shape == grads.shape, f\"Parameters shape {params.shape} does not match gradients shape {grads.shape}\"\n",
    "        params -= self.learning_rate * grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed26f1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put everything together in a very simplistic training loop \n",
    "# Note: better training infrastructure can be found in this notebook further on\n",
    "perform_simple_training = False\n",
    "\n",
    "\n",
    "def simple_train(model: Sequential, loss_fn: Loss, optimizer: Optimizer, x_train: np.ndarray, y_train: np.ndarray, batch_size: int = 32, epochs: int = 1000) -> None:\n",
    "    '''\n",
    "    Train the model using the specified loss function and optimizer. No cross-validation is performed.\n",
    "    The training loop consists of the following steps:\n",
    "    1. Forward pass: Compute the predicted labels using the model.\n",
    "    2. Compute the loss using the loss function.\n",
    "    3. Backward pass: Compute the gradients of the loss with respect to the model parameters.\n",
    "    4. Update the model parameters using the optimizer.\n",
    "    5. Repeat steps 1-4 for the specified number of epochs.\n",
    "    6. Print the loss every 100 epochs.\n",
    "    7. Return the trained model.\n",
    "    Args:\n",
    "        model (Sequential): The model to be trained.\n",
    "        loss_fn (Loss): The loss function to be used.\n",
    "        optimizer (Optimizer): The optimizer to be used.\n",
    "        x_train (np.ndarray): Training data. The shape should be (n_samples, n_features).\n",
    "        y_train (np.ndarray): Training labels. The shape should be (n_samples, n_classes).\n",
    "        batch_size (int): Batch size for training. Default is 32.\n",
    "        epochs (int): Number of epochs for training. Default is 1000.\n",
    "    Returns:\n",
    "        None\n",
    "    '''\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        # go in batches\n",
    "        for i in range(0, x_train.shape[0], batch_size):\n",
    "            x_batch = x_train[i:i + batch_size]\n",
    "            y_batch = y_train[i:i + batch_size]\n",
    "            \n",
    "            # Forward pass\n",
    "            y_pred = model.forward(x_batch)\n",
    "            \n",
    "            # Compute loss\n",
    "            batch_loss, probs = loss_fn.forward(y_batch, y_pred)\n",
    "            epoch_loss += batch_loss\n",
    "            \n",
    "            # Backward pass\n",
    "            grad = loss_fn.backward(y_batch, probs)\n",
    "            model.backward(grad)\n",
    "            \n",
    "            # Update parameters\n",
    "            for layer in model.layers:\n",
    "                if isinstance(layer, Linear):\n",
    "                    optimizer.step(layer.weights, layer.grad_weights)\n",
    "                    optimizer.step(layer.bias, layer.grad_bias)\n",
    "        # Average loss for the epoch\n",
    "        epoch_loss /= (x_train.shape[0] // batch_size)\n",
    "\n",
    "        # Print loss every 100 epochs\n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {epoch_loss}\")\n",
    "            \n",
    "\n",
    "#Define a simple model and apply it to spiral dataset\n",
    "model = Sequential([\n",
    "    Linear(2, 32),\n",
    "    ReLU(),\n",
    "    Linear(32, 5),\n",
    "])\n",
    "loss_fn = CrossEntropySoftMax()\n",
    "optimizer = SGD(learning_rate=0.01)\n",
    "\n",
    "if perform_simple_training: \n",
    "    # Train the model\n",
    "    x_train, y_train = generate_spiral_data(1000, 5)\n",
    "    x_test, y_test = generate_spiral_data(200, 5)\n",
    "    simple_train(model, loss_fn, optimizer, x_train, y_train, epochs=1000, batch_size=10)\n",
    "    # Test the model\n",
    "    y_pred = model.forward(x_test)\n",
    "    predicted_labels = np.argmax(y_pred, axis=1)\n",
    "    true_labels = np.argmax(y_test, axis=1)\n",
    "    accuracy = np.mean(predicted_labels == true_labels)\n",
    "    print(f\"Test Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9758906f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset and DataLoader classes to wrap the data and operate on batches\n",
    "class Dataset:\n",
    "    '''\n",
    "    Dataset class to hold the data and labels.\n",
    "    It provides methods to access the data and labels by index.\n",
    "    '''\n",
    "    def __init__(self, x: np.ndarray, y: np.ndarray)-> None:\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, index: Union[int, np.ndarray]) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        assert isinstance(index, (int, np.ndarray)), \"Index must be an integer or a numpy array.\"\n",
    "        return self.x[index], self.y[index]\n",
    "    \n",
    "class DataLoader:\n",
    "    '''\n",
    "    DataLoader class to load the data in batches.\n",
    "    It provides methods to iterate over the data in batches.\n",
    "    '''\n",
    "    def __init__(self, dataset: Dataset, indices: Optional[np.ndarray] = None, batch_size: int = 32, shuffle: bool = False) -> None:\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.indices = indices if indices is not None else np.arange(len(dataset))\n",
    "        self.current_index = 0\n",
    "\n",
    "    def __iter__(self) -> 'DataLoader':\n",
    "        self.current_index = 0\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indices)\n",
    "        return self\n",
    "\n",
    "    def __next__(self)-> Tuple[np.ndarray, np.ndarray]:\n",
    "        if self.current_index >= len(self.indices):\n",
    "            raise StopIteration\n",
    "        start_index = self.current_index\n",
    "        end_index = min(start_index + self.batch_size, len(self.indices))\n",
    "        batch_indices = self.indices[start_index:end_index]\n",
    "        x_batch, y_batch = self.dataset[batch_indices]\n",
    "        self.current_index += self.batch_size\n",
    "        return x_batch, y_batch\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return int(np.ceil(len(self.indices) / self.batch_size))\n",
    "    \n",
    "    @staticmethod\n",
    "    def holdout_split(dataset: Dataset, test_size: float = 0.2) -> Tuple['DataLoader', 'DataLoader']:\n",
    "        \"\"\"\n",
    "        Splits the dataset into training and testing sets.\n",
    "        Args:\n",
    "            dataset (Dataset): The dataset to split.\n",
    "            test_size (float): The proportion of the dataset to include in the test split.\n",
    "        Returns:\n",
    "            DataLoader: Loader for the training portion of the dataset.\n",
    "            DataLoader: Loader for the testing portion of the dataset.\n",
    "        \"\"\"\n",
    "        assert 0 < test_size < 1, \"test_size must be between 0 and 1.\"\n",
    "        indices = np.arange(len(dataset))\n",
    "        np.random.shuffle(indices)\n",
    "        split_index = int(len(dataset) * (1 - test_size))\n",
    "        train_indices = indices[:split_index]\n",
    "        test_indices = indices[split_index:]\n",
    "        return DataLoader(dataset, train_indices), DataLoader(dataset, test_indices)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbc8c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate scheduler\n",
    "\n",
    "class LRScheduler(ABC):\n",
    "    '''\n",
    "    Abstract base class for learning rate schedulers.\n",
    "    Each scheduler should implement the get_lr method to compute the current learning rate.\n",
    "    '''\n",
    "    def __init__(self, initial_lr: float) -> None:\n",
    "        assert initial_lr > 0, \"Initial learning rate must be positive\"\n",
    "        self.initial_lr = initial_lr\n",
    "        self.current_step = 0\n",
    "        \n",
    "    @abstractmethod\n",
    "    def get_lr(self) -> float:\n",
    "        \"\"\"Calculate the learning rate for the current step\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def step(self) -> None:\n",
    "        \"\"\"Increment the step counter\"\"\"\n",
    "        self.current_step += 1\n",
    "\n",
    "class StepLRScheduler(LRScheduler):\n",
    "    \"\"\"Decays learning rate by gamma every step_size epochs\"\"\"\n",
    "    def __init__(self, initial_lr: float, step_size: int, gamma: float = 0.1):\n",
    "        super().__init__(initial_lr)\n",
    "        self.step_size = step_size\n",
    "        self.gamma = gamma\n",
    "        \n",
    "    def get_lr(self) -> float:\n",
    "        return self.initial_lr * (self.gamma ** (self.current_step // self.step_size))\n",
    "\n",
    "class ExponentialLRScheduler(LRScheduler):\n",
    "    \"\"\"Exponentially decays learning rate by gamma every epoch\"\"\"\n",
    "    def __init__(self, initial_lr: float, gamma: float = 0.95)-> None:\n",
    "        super().__init__(initial_lr)\n",
    "        self.gamma = gamma\n",
    "        \n",
    "    def get_lr(self) -> float:\n",
    "        return self.initial_lr * (self.gamma ** self.current_step)\n",
    "    \n",
    "class WarmupLRScheduler(LRScheduler):\n",
    "    def __init__(self, initial_lr: float, warmup_epochs: int = 50) -> None:\n",
    "        super().__init__(initial_lr)\n",
    "        self.warmup_epochs = warmup_epochs\n",
    "        \n",
    "    def get_lr(self) -> float:\n",
    "        if self.current_step < self.warmup_epochs:\n",
    "            return self.initial_lr * (self.current_step / self.warmup_epochs)\n",
    "        return self.initial_lr * 0.5 * (1 + np.cos(np.pi * (self.current_step - self.warmup_epochs) / (1000 - self.warmup_epochs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca70c15a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# class TrainingVisualizer that stores the training history and plots the loss and accuracy\n",
    "\n",
    "class TrainingVisualizer:\n",
    "    '''\n",
    "    TrainingVisualizer class to store the training history and plot the loss and accuracy.\n",
    "    It provides methods to update the training history and plot the loss and accuracy.\n",
    "    '''\n",
    "    def __init__(self)->None:\n",
    "        self.history = {\n",
    "            'loss': [],\n",
    "            'val_loss': [],\n",
    "            'train_acc': [],\n",
    "            'val_acc': []\n",
    "        }\n",
    "        self.grid = None\n",
    "        self.grid_coords = None\n",
    "        \n",
    "    def update(self, loss: float, val_loss: float, train_acc: float, val_acc: float):\n",
    "        '''\n",
    "        Update the training history with the current loss and accuracy.\n",
    "        Args:   \n",
    "            loss (float): Current loss.\n",
    "            val_loss (float): Current validation loss.\n",
    "            train_acc (float): Current training accuracy.\n",
    "            val_acc (float): Current validation accuracy.\n",
    "        Returns:\n",
    "            None\n",
    "        '''\n",
    "        self.history['loss'].append(loss)\n",
    "        self.history['val_loss'].append(val_loss)\n",
    "        self.history['train_acc'].append(train_acc)\n",
    "        self.history['val_acc'].append(val_acc)\n",
    "\n",
    "    def plot_metrics_history(self) -> None:\n",
    "        '''\n",
    "        Plot the training history.\n",
    "        It plots the loss and accuracy for both training and validation sets.\n",
    "        Returns:\n",
    "            None\n",
    "        '''\n",
    "        assert len(self.history['loss']) > 0, \"No training history to plot.\"\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "        \n",
    "        epochs = range(len(self.history['loss']))\n",
    "        # Plot loss\n",
    "        ax1.plot(epochs, self.history['loss'], 'b-', label='Train Loss')\n",
    "        ax1.plot(epochs, self.history['val_loss'], 'r-', label='Val Loss')\n",
    "        ax1.set_title('Loss')\n",
    "        ax1.set_xlabel('Epochs')\n",
    "        ax1.set_ylabel('Loss')\n",
    "        ax1.legend()\n",
    "        \n",
    "        # Plot accuracy\n",
    "        ax2.plot(epochs, self.history['train_acc'], 'b-', label='Train Accuracy')\n",
    "        ax2.plot(epochs, self.history['val_acc'], 'r-', label='Val Accuracy')\n",
    "        ax2.set_title('Accuracy')\n",
    "        ax2.set_xlabel('Epochs')\n",
    "        ax2.set_ylabel('Accuracy (%)')\n",
    "        ax2.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()     \n",
    "    \n",
    "    def plot_decision_boundary(self, model: Sequential, x_train: np.ndarray, y_train: np.ndarray, ax: Optional[plt.Axes] = None) -> None:\n",
    "        if ax is None:\n",
    "            fig, ax = plt.subplots(figsize=(8, 6))\n",
    "        \n",
    "        # Create grid first time only\n",
    "        if self.grid is None:\n",
    "            # Extend bounds a bit further for better visualization\n",
    "            x_min, x_max = x_train[:, 0].min() - 1.0, x_train[:, 0].max() + 1.0\n",
    "            y_min, y_max = x_train[:, 1].min() - 1.0, x_train[:, 1].max() + 1.0\n",
    "        \n",
    "            # Increase grid resolution for smoother boundaries\n",
    "            xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200),\n",
    "                            np.linspace(y_min, y_max, 200))\n",
    "            self.grid = np.c_[xx.ravel(), yy.ravel()]\n",
    "            self.grid_coords = (xx, yy)\n",
    "    \n",
    "        # Get predictions for grid points\n",
    "        grid_predictions = model.forward(self.grid)\n",
    "        grid_predictions = np.argmax(grid_predictions, axis=1)\n",
    "    \n",
    "        # Plot decision boundary with better aesthetics\n",
    "        ax.contourf(self.grid_coords[0], self.grid_coords[1], \n",
    "                 grid_predictions.reshape(self.grid_coords[0].shape),\n",
    "                 alpha=0.15, cmap='viridis', levels=np.arange(4)-0.5)\n",
    "    \n",
    "        # Add contour lines to highlight boundaries\n",
    "        ax.contour(self.grid_coords[0], self.grid_coords[1],\n",
    "                grid_predictions.reshape(self.grid_coords[0].shape),\n",
    "                colors='black', alpha=0.3, linewidths=0.5)\n",
    "    \n",
    "        # Plot training points with better visibility\n",
    "        scatter = ax.scatter(x_train[:, 0], x_train[:, 1], \n",
    "                         c=np.argmax(y_train, axis=1), \n",
    "                         cmap='viridis',\n",
    "                         edgecolors='white',\n",
    "                         s=20,\n",
    "                         alpha=0.6,  # Some transparency\n",
    "                         linewidth=0.5)\n",
    "    \n",
    "        if hasattr(ax, 'figure'):\n",
    "            ax.figure.colorbar(scatter, ax=ax, label='Class')\n",
    "        \n",
    "        ax.set_xlabel('X', fontsize=12)\n",
    "        ax.set_ylabel('Y', fontsize=12)\n",
    "        ax.set_title('Decision Boundary', fontsize=14, pad=10)\n",
    "    \n",
    "        # Make plot more aesthetic\n",
    "        ax.grid(True, alpha=0.2)\n",
    "        plt.tight_layout()\n",
    "        if ax is None:\n",
    "            plt.show()\n",
    "        \n",
    "    def weights_gradients_heatmap(self, model: Sequential, optimizer: Optimizer, ax: Optional[plt.Axes]=None) -> None:\n",
    "        '''\n",
    "        Plot the weights and their updates during training.\n",
    "        Args:\n",
    "            model: Sequential model to visualize\n",
    "            optimizer: Optimizer instance to calculate updates\n",
    "            ax: Matplotlib axes to plot on, if None a new figure is created\n",
    "        Returns:\n",
    "            None\n",
    "        '''\n",
    "        \n",
    "        # Get all Linear layers\n",
    "        linear_layers = [(i, layer) for i, layer in enumerate(model.layers) \n",
    "                          if isinstance(layer, Linear)]\n",
    "    \n",
    "        if not linear_layers:\n",
    "            print(\"No linear layers to visualize.\")\n",
    "            return\n",
    "        \n",
    "        num_layers = len(linear_layers)\n",
    "    \n",
    "        # Create figure with subplots for each layer\n",
    "        if ax is None:\n",
    "            fig = plt.figure(figsize=(12, 4 * num_layers + 1))  # Add extra space for title\n",
    "            fig.patch.set_visible(False)\n",
    "            gs = fig.add_gridspec(num_layers, 2, hspace=0.6, height_ratios=[1]*num_layers)\n",
    "            axes = np.empty((num_layers, 2), dtype=object)\n",
    "            for i in range(num_layers):\n",
    "                axes[i, 0] = fig.add_subplot(gs[i, 0])\n",
    "                axes[i, 1] = fig.add_subplot(gs[i, 1])\n",
    "        else:\n",
    "            fig = ax.get_figure()\n",
    "            gs = GridSpecFromSubplotSpec(num_layers, 2, subplot_spec=ax.get_subplotspec(), hspace=0.6)\n",
    "            axes = np.empty((num_layers, 2), dtype=object)\n",
    "            for i in range(num_layers):\n",
    "                axes[i, 0] = fig.add_subplot(gs[i, 0])\n",
    "                axes[i, 1] = fig.add_subplot(gs[i, 1])\n",
    "        \n",
    "        for i, (layer_num, layer) in enumerate(linear_layers):\n",
    "            # Left plot - weights\n",
    "            weights_norm = layer.weights / np.abs(layer.weights).max()\n",
    "            im1 = axes[i, 0].imshow(weights_norm, cmap='RdBu', vmin=-1, vmax=1)\n",
    "            axes[i, 0].set_title(f'Layer {layer_num} Weights\\nMax abs value: {np.abs(layer.weights).max():.4f}', \n",
    "                            pad=10)\n",
    "            fig.colorbar(im1, ax=axes[i, 0])\n",
    "\n",
    "            # Right plot - updates\n",
    "            if layer.grad_weights is not None:\n",
    "                update = optimizer.learning_rate * layer.grad_weights\n",
    "                update_norm = update / np.abs(update).max() if np.abs(update).max() > 0 else update\n",
    "                im2 = axes[i, 1].imshow(update_norm, cmap='RdBu', vmin=-1, vmax=1)\n",
    "                axes[i, 1].set_title(f'Layer {layer_num} Updates (lr={optimizer.learning_rate:.6f})\\nMax abs value: {np.abs(update).max():.4f}', \n",
    "                                pad=10)\n",
    "                fig.colorbar(im2, ax=axes[i, 1])\n",
    "\n",
    "            # Add labels with consistent spacing\n",
    "            for ax in [axes[i, 0], axes[i, 1]]:\n",
    "                ax.set_xlabel('Output features', labelpad=10)\n",
    "                ax.set_ylabel('Input features', labelpad=10)\n",
    "            \n",
    "            # Adjust tick labels if needed\n",
    "            for ax in [axes[i, 0], axes[i, 1]]:\n",
    "                ax.tick_params(axis='both', which='major', labelsize=8)\n",
    "\n",
    "        plt.suptitle('Weight Values and Their Updates', y=1.02, fontsize=14)\n",
    "    \n",
    "        if ax is None:\n",
    "            plt.tight_layout(rect=[0, 0.03, 1, 0.95])  # Adjust layout to prevent overlap\n",
    "            plt.show()\n",
    "        \n",
    "    \n",
    "    \n",
    "        \n",
    "    def plot_loss_landscape(self, model: Sequential, loader: DataLoader, loss_fn: Loss, ax: Optional[plt.Axes]=None)->None:\n",
    "        \"\"\"Visualize loss landscape around current weights\"\"\"\n",
    "        if ax is None:\n",
    "            fig, ax = plt.subplots(figsize=(8, 6))\n",
    "        \n",
    "        losses = []\n",
    "        epsilons = np.linspace(-1, 1, 20)\n",
    "    \n",
    "        # Store original weights\n",
    "        original_weights = [(layer.weights.copy(), layer.bias.copy()) \n",
    "                       for layer in model.layers if isinstance(layer, Linear)]\n",
    "    \n",
    "        for eps in epsilons:\n",
    "            # Perturb weights\n",
    "            for layer, (w, b) in zip([l for l in model.layers if isinstance(l, Linear)], \n",
    "                                original_weights):\n",
    "                layer.weights = w + eps * np.random.randn(*w.shape) * 0.1\n",
    "                layer.bias = b + eps * np.random.randn(*b.shape) * 0.1\n",
    "        \n",
    "            # Compute loss\n",
    "            total_loss = 0\n",
    "            n_samples = 0\n",
    "            for x_batch, y_batch in loader:\n",
    "                y_pred = model.forward(x_batch)\n",
    "                loss, _ = loss_fn.forward(y_batch, y_pred)\n",
    "                total_loss += loss * len(x_batch)\n",
    "                n_samples += len(x_batch)\n",
    "            losses.append(total_loss / n_samples)\n",
    "    \n",
    "            # Restore original weights\n",
    "            for layer, (w, b) in zip([l for l in model.layers if isinstance(l, Linear)], \n",
    "                            original_weights):\n",
    "                layer.weights = w\n",
    "                layer.bias = b\n",
    "    \n",
    "        ax.plot(epsilons, losses)\n",
    "        ax.set_xlabel('Perturbation magnitude')\n",
    "        ax.set_ylabel('Loss')\n",
    "        ax.set_title('Loss Landscape')\n",
    "        ax.grid(True)\n",
    "        if ax is None:\n",
    "            plt.show()\n",
    "        \n",
    "\n",
    "class KFoldVisualizer(TrainingVisualizer):\n",
    "    \"\"\"Extended visualizer for k-fold cross validation\"\"\"\n",
    "    def __init__(self, k_folds: int) -> None:\n",
    "        super().__init__()\n",
    "        self.k_folds = k_folds\n",
    "        self.fold_histories = []\n",
    "        \n",
    "    def add_fold_history(self, fold_history: dict) ->None:\n",
    "        \"\"\"Store history for one fold\"\"\"\n",
    "        self.fold_histories.append(fold_history)\n",
    "    \n",
    "    def plot_k_fold_results(self)-> None:\n",
    "        \"\"\"Plot aggregated results across folds\"\"\"\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "        \n",
    "        # Plot individual fold histories\n",
    "        for i, hist in enumerate(self.fold_histories):\n",
    "            ax1.plot(hist['val_loss'], alpha=0.3, label=f'Fold {i+1}')\n",
    "            ax2.plot(hist['val_acc'], alpha=0.3)\n",
    "        \n",
    "        # Plot mean Â± std\n",
    "        val_losses = np.array([h['val_loss'] for h in self.fold_histories])\n",
    "        val_accs = np.array([h['val_acc'] for h in self.fold_histories])\n",
    "        \n",
    "        epochs = range(len(val_losses[0]))\n",
    "        mean_loss = np.mean(val_losses, axis=0)\n",
    "        std_loss = np.std(val_losses, axis=0)\n",
    "        mean_acc = np.mean(val_accs, axis=0)\n",
    "        std_acc = np.std(val_accs, axis=0)\n",
    "        \n",
    "        ax1.plot(epochs, mean_loss, 'r-', label='Mean Loss', linewidth=2)\n",
    "        ax1.fill_between(epochs, mean_loss-std_loss, mean_loss+std_loss, alpha=0.2)\n",
    "        ax1.set_title(f'Validation Loss Across {self.k_folds} Folds')\n",
    "        ax1.set_xlabel('Epochs')\n",
    "        ax1.set_ylabel('Loss')\n",
    "        ax1.legend()\n",
    "        \n",
    "        ax2.plot(epochs, mean_acc, 'r-', label='Mean Accuracy', linewidth=2)\n",
    "        ax2.fill_between(epochs, mean_acc-std_acc, mean_acc+std_acc, alpha=0.2)\n",
    "        ax2.set_title(f'Validation Accuracy Across {self.k_folds} Folds')\n",
    "        ax2.set_xlabel('Epochs')\n",
    "        ax2.set_ylabel('Accuracy (%)')\n",
    "        ax2.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44698a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CrossValidator class to perform k-fold cross-validation and other validation strategies\n",
    "# In progress\n",
    "\n",
    "class CrossValidator:\n",
    "    def __init__(self, validation_strategy: str = \"k-fold\", **kwargs) -> None:\n",
    "        assert validation_strategy in [\"k-fold\", \"leave-one-out\", \"stratified\"],\\\n",
    "            \"Validation strategy must be one of: 'k-fold', 'leave-one-out', 'stratified'\"\n",
    "        self.validation_strategy = validation_strategy\n",
    "        self.kwargs = kwargs  # Store parameters like k, etc.\n",
    "        self.random_state = np.random.RandomState(42)  # Add separate random state\n",
    "\n",
    "\n",
    "    def get_folds(self, dataset: Dataset) -> List[Tuple[np.ndarray, np.ndarray]]:\n",
    "        \"\"\"Returns list of (train_indices, val_indices) for the chosen strategy\"\"\"\n",
    "        if self.validation_strategy == \"k-fold\":\n",
    "            k = self.kwargs.get('k', 5)\n",
    "            indices = np.arange(len(dataset))\n",
    "            self.random_state.shuffle(indices)\n",
    "            fold_size = len(dataset) // k\n",
    "            folds = []\n",
    "            for i in range(k):\n",
    "                val_idx = indices[i*fold_size:(i+1)*fold_size]\n",
    "                train_idx = np.concatenate([indices[:i*fold_size], \n",
    "                                          indices[(i+1)*fold_size:]])\n",
    "                folds.append((train_idx, val_idx))\n",
    "            return folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804e0e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Trainer that governs the training process, using the DataLoader, ValidationStrategy, and Optimizer\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, model: Sequential, loss_fn: Loss, optimizer: Optimizer) -> None:\n",
    "        self.model = model\n",
    "        self.loss_fn = loss_fn\n",
    "        self.optimizer = optimizer\n",
    "        self.visualizer = TrainingVisualizer()\n",
    "\n",
    "    def train(self, train_loader: DataLoader, val_loader: DataLoader, epochs: int = 1000,  show_plots_logs:bool = True, log_interval:int = 200, patience: int = 20, min_delta: float = 1e-4, lr_scheduler: Optional[LRScheduler] = None, debug: bool = False, **kwargs) -> Dict[str, List[float]]:\n",
    "        '''\n",
    "        Train the model using the specified loss function and optimizer.\n",
    "        The training loop consists of the following steps:\n",
    "        1. Split the dataset into training and validation sets.\n",
    "        2. For each epoch:\n",
    "            a. Iterate over the training set in batches.\n",
    "            b. Forward pass: Compute the predicted labels using the model.\n",
    "            c. Compute the loss using the loss function.\n",
    "            d. Backward pass: Compute the gradients of the loss with respect to the model parameters.\n",
    "            e. Update the model parameters using the optimizer.\n",
    "            f. Print the loss every 100 epochs.\n",
    "        3. Validate the model using the validation set.\n",
    "        4. Return the trained model.\n",
    "        Args:\n",
    "            train_loader (DataLoader): The DataLoader for the training set.\n",
    "            val_loader (DataLoader): The DataLoader for the validation set.\n",
    "            epochs (int): Number of epochs for training. Default is 1000.\n",
    "            show_plots_logs (bool): Whether to show the plots and output statistics regularly during training. Default is True.\n",
    "            log_interval (int): Interval for showing plots and statistics. Default is 200.\n",
    "            patience (int): Number of epochs to wait for improvement before early stopping. Default is 10.\n",
    "            min_delta (float): Minimum change in the validation loss to qualify as an improvement. Default is 1e-4.\n",
    "            lr_scheduler (LRScheduler): Learning rate scheduler to adjust the learning rate during training.\n",
    "            **kwargs: Additional arguments (e.g., batch_size)\n",
    "        Returns:\n",
    "            dict: Training history containing loss, validation loss, training accuracy, and validation accuracy.\n",
    "        '''\n",
    "        \n",
    "        if debug:\n",
    "            stats = {}\n",
    "            for i, layer in enumerate(self.model.layers):\n",
    "                if isinstance(layer, Linear):\n",
    "                    stats[f'layer_{i}'] = {\n",
    "                        'weight_norm': np.linalg.norm(layer.weights),\n",
    "                        'grad_norm': np.linalg.norm(layer.grad_weights) if layer.grad_weights is not None else 0\n",
    "                    }\n",
    "            weight_stats.append(stats)\n",
    "        \n",
    "        def validate_metrics(loss: float, val_loss: float, acc: float) -> None:\n",
    "            \"\"\"Helper function to validate metrics\"\"\"\n",
    "            if np.isnan(loss) or np.isinf(loss):\n",
    "                raise ValueError(f\"Training loss is {loss}\")\n",
    "            if np.isnan(val_loss) or np.isinf(val_loss):\n",
    "                raise ValueError(f\"Validation loss is {val_loss}\")\n",
    "            if acc < 0 or acc > 100:\n",
    "                raise ValueError(f\"Invalid accuracy value: {acc}\")\n",
    "        \n",
    "        # If no scheduler provided, create default one\n",
    "        if lr_scheduler is None:\n",
    "            lr_scheduler = ExponentialLRScheduler(0.01, gamma=0.99)\n",
    "            \n",
    "        epoch_times = [] # List to store training times per epoch for logging\n",
    "        weight_stats = [] # List to store weight statistics for debugging\n",
    "        \n",
    "        # Initialize best validation loss, best model state and patiance counter for early stopping\n",
    "        best_val_loss = float('inf')\n",
    "        best_model_state = None\n",
    "        patience_counter = 0\n",
    "        \n",
    "        # Training loop\n",
    "        for epoch in range(epochs):\n",
    "            if debug:\n",
    "                stats = {}\n",
    "                for i, layer in enumerate(self.model.layers):\n",
    "                    if isinstance(layer, Linear):\n",
    "                        stats[f'layer_{i}'] = {\n",
    "                            'weight_mean': layer.weights.mean(),\n",
    "                            'weight_std': layer.weights.std(),\n",
    "                            'weight_max': np.abs(layer.weights).max(),\n",
    "                            'grad_max': np.abs(layer.grad_weights).max() if layer.grad_weights is not None else 0\n",
    "                        }\n",
    "                weight_stats.append(stats)\n",
    "\n",
    "            start_time = time.time()\n",
    "            epoch_loss = 0\n",
    "            n_samples = 0\n",
    "            \n",
    "            current_lr = lr_scheduler.get_lr()\n",
    "           # print(f\"Epoch {epoch}, LR: {current_lr:.6f}\") if epoch % log_interval == 0 else None\n",
    "            \n",
    "            self.optimizer.update_learning_rate(current_lr)\n",
    "            \n",
    "            # Monitor gradients before updates\n",
    "            max_grad = 0\n",
    "            \n",
    "            for x_batch, y_batch in train_loader:\n",
    "                \n",
    "                # Forward pass\n",
    "                y_pred = self.model.forward(x_batch)\n",
    "                \n",
    "                # Compute loss\n",
    "                loss, probs = self.loss_fn.forward(y_batch, y_pred)\n",
    "                \n",
    "                # Check for valid predictions\n",
    "                if np.any(np.isnan(y_pred)) or np.any(np.isinf(y_pred)):\n",
    "                    raise ValueError(\"Invalid predictions detected\")\n",
    "                \n",
    "                epoch_loss += loss * x_batch.shape[0]\n",
    "                n_samples += x_batch.shape[0]\n",
    "                \n",
    "                # Backward pass with gradient clipping\n",
    "                grad = self.loss_fn.backward(y_batch, probs)\n",
    "                max_grad = max(max_grad, np.abs(grad).max())\n",
    "                grad = np.clip(grad, -1.0, 1.0)\n",
    "                self.model.backward(grad)\n",
    "                \n",
    "                # Update parameters\n",
    "                for layer in self.model.layers:\n",
    "                    if isinstance(layer, Linear):\n",
    "                        if np.any(np.isnan(layer.grad_weights)) or np.any(np.isinf(layer.grad_weights)):\n",
    "                            raise ValueError(\"Invalid gradients detected\")\n",
    "                        layer.grad_weights = np.clip(layer.grad_weights, -1.0, 1.0)\n",
    "                        layer.grad_bias = np.clip(layer.grad_bias, -1.0, 1.0)\n",
    "                        self.optimizer.step(layer.weights, layer.grad_weights)\n",
    "                        self.optimizer.step(layer.bias, layer.grad_bias)\n",
    "            \n",
    "            #Step the learning rate scheduler\n",
    "            lr_scheduler.step()\n",
    "            \n",
    "            epoch_loss /= n_samples\n",
    "            \n",
    "            # compute and validate the metrics for this epoch\n",
    "            val_loss = self.validate(val_loader)\n",
    "            train_acc = self.compute_accuracy(train_loader)\n",
    "            val_acc = self.compute_accuracy(val_loader)\n",
    "            \n",
    "            try:\n",
    "                validate_metrics(epoch_loss, val_loss, train_acc)\n",
    "            except ValueError as e:\n",
    "                print(f\"Error at epoch {epoch}:\")\n",
    "                print(f\"Max gradient magnitude: {max_grad}\")\n",
    "                print(f\"Current learning rate: {current_lr}\")\n",
    "                raise e\n",
    "            \n",
    "            self.visualizer.update(epoch_loss, val_loss, train_acc, val_acc)\n",
    "            epoch_times.append(time.time() - start_time)\n",
    "            # Early stopping check\n",
    "            if val_loss < best_val_loss*(1 - min_delta):\n",
    "                best_val_loss = val_loss\n",
    "                best_model_state = copy.deepcopy([(layer.weights.copy(), layer.bias.copy()) \n",
    "                        for layer in self.model.layers \n",
    "                        if isinstance(layer, Linear)])\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping triggered after {epoch} epochs\")\n",
    "                # Restore best model\n",
    "                linear_layers = [l for l in self.model.layers if isinstance(l, Linear)]\n",
    "                for layer, (weights, bias) in zip(linear_layers, best_model_state):\n",
    "                    layer.weights = weights.copy()\n",
    "                    layer.bias = bias.copy()\n",
    "                    \n",
    "                # Truncate history at early stopping point\n",
    "                for key in self.visualizer.history:\n",
    "                    self.visualizer.history[key] = self.visualizer.history[key][:epoch + 1]\n",
    "                \n",
    "                break\n",
    "            \n",
    "            # Print loss every log_interval epochs\n",
    "            if show_plots_logs and epoch % log_interval == 0:\n",
    "                \n",
    "                # Create a figure with subplots for all visualizations\n",
    "                fig = plt.figure(figsize=(15, 12))\n",
    "                fig.set_facecolor('none')\n",
    "                gs = plt.GridSpec(3, 2, height_ratios=[1, 1, 1])\n",
    "            \n",
    "                # Decision boundary\n",
    "                ax1 = fig.add_subplot(gs[0, 0])\n",
    "                self.visualizer.plot_decision_boundary(self.model, train_loader.dataset.x, \n",
    "                                                train_loader.dataset.y, ax=ax1)\n",
    "            \n",
    "                # Loss landscape\n",
    "                ax2 = fig.add_subplot(gs[0, 1])\n",
    "                self.visualizer.plot_loss_landscape(self.model, val_loader, \n",
    "                                              self.loss_fn, ax=ax2)\n",
    "\n",
    "                # Weights and gradients heatmap\n",
    "                ax3 = fig.add_subplot(gs[1:, :])  # Use slice notation instead of tuple\n",
    "                self.visualizer.weights_gradients_heatmap(self.model, \n",
    "                                        self.optimizer, \n",
    "                                        ax=ax3)\n",
    "            \n",
    "            \n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "                    \n",
    "                # Print essential metrics\n",
    "                print(f\"Epoch {epoch}: Loss={epoch_loss:.4f}, Val Loss={val_loss:.4f}, \"\n",
    "                f\"Acc={train_acc:.1f}%, Val Acc={val_acc:.1f}%\")\n",
    "\n",
    "            \n",
    "                \n",
    "                \n",
    "        # Plot the metrics history during training\n",
    "        self.visualizer.plot_metrics_history()\n",
    "        return self.visualizer.history\n",
    "                \n",
    "    def validate(self, val_loader: DataLoader) -> float:\n",
    "        '''\n",
    "        Validate the model using the validation set.\n",
    "        The validation loop consists of the following steps:\n",
    "        1. Iterate over the validation set in batches.\n",
    "        2. Forward pass: Compute the predicted labels using the model.\n",
    "        3. Compute the loss using the loss function.\n",
    "        4. Return the average loss for the validation set.\n",
    "        Args:\n",
    "            val_loader (DataLoader): The DataLoader for the validation set.\n",
    "        Returns:\n",
    "            float: The average loss for the validation set.\n",
    "        '''\n",
    "        \n",
    "        val_loss = 0\n",
    "        n_samples = 0\n",
    "        for x_val, y_val in val_loader:\n",
    "            y_val_pred = self.model.forward(x_val)\n",
    "            loss = self.loss_fn.forward(y_val, y_val_pred)[0]\n",
    "            val_loss += loss * x_val.shape[0]\n",
    "            n_samples += x_val.shape[0]\n",
    "        return val_loss / n_samples\n",
    "    \n",
    "    def compute_accuracy(self, loader: DataLoader) -> float:\n",
    "        '''\n",
    "        Compute the accuracy of the model on the given DataLoader.\n",
    "        The accuracy is defined as the number of correct predictions divided by the total number of predictions.\n",
    "        Args:\n",
    "            loader (DataLoader): The DataLoader for the dataset.\n",
    "        Returns:\n",
    "            float: The accuracy of the model on the dataset.\n",
    "        ''' \n",
    "        total_correct = 0\n",
    "        total_samples = 0\n",
    "        smooth_window = 5  \n",
    "        running_acc = []\n",
    "        \n",
    "        for x_batch, y_batch in loader:\n",
    "            pred = self.model.forward(x_batch)\n",
    "            batch_correct = np.sum(np.argmax(pred, axis=1) == np.argmax(y_batch, axis=1))\n",
    "            total_correct += batch_correct\n",
    "            total_samples += len(y_batch)\n",
    "            acc = 100 * batch_correct / len(y_batch)\n",
    "            running_acc.append(acc)\n",
    "            if len(running_acc) > smooth_window:\n",
    "                running_acc.pop(0)\n",
    "        \n",
    "        return np.mean(running_acc) if running_acc else 0.0  # Return mean of the last few accuracies\n",
    "    \n",
    "    \n",
    "    def train_with_cv(self, dataset: Dataset, cv: CrossValidator, **kwargs) -> Dict[str, Union[float, List[float], Sequential, KFoldVisualizer]]:\n",
    "        \"\"\"Train with cross-validation and return the best model\"\"\"\n",
    "        cv_visualizer = KFoldVisualizer(len(cv.get_folds(dataset)))\n",
    "        fold_scores = []\n",
    "        best_model = None\n",
    "        best_score = float('inf')\n",
    "        \n",
    " \n",
    "                \n",
    "        # Store initial model architecture\n",
    "        initial_architecture = [(layer.__class__, layer.input_dim, layer.output_dim) \n",
    "                         if isinstance(layer, Linear) else layer.__class__ \n",
    "                          for layer in self.model.layers]\n",
    "        \n",
    "        # Create fresh learning rate scheduler for each fold with same parameters\n",
    "        lr_params = {'initial_lr': kwargs.get('lr_scheduler').initial_lr, \n",
    "                     'warmup_epochs': kwargs.get('lr_scheduler').warmup_epochs}\n",
    "        \n",
    "        #if lr_scheduler is None:\n",
    "        #    lr_scheduler = ExponentialLRScheduler(initial_lr=0.01, gamma=0.999)\n",
    "        #kwargs['lr_scheduler'] = lr_scheduler\n",
    "        \n",
    "\n",
    "        # Set consistent batch size\n",
    "        #kwargs['batch_size'] = min(32, len(dataset) // 10)  # Smaller batches for stability\n",
    "\n",
    "        \n",
    "        for fold_idx, (train_idx, val_idx) in enumerate(cv.get_folds(dataset)):\n",
    "            print(f\"\\nTraining Fold {fold_idx + 1}\")\n",
    "            \n",
    "            # Reset visualization history for each fold\n",
    "            self.visualizer = TrainingVisualizer()\n",
    "        \n",
    "            ## Create fresh model for each fold\n",
    "            #self.model = copy.deepcopy(self.model)\n",
    "            \n",
    "            np.random.seed(42 + fold_idx)  # Consistent but different for each fold\n",
    "            \n",
    "            # Create fresh model with consistent initialization        \n",
    "            layers = []\n",
    "            for layer_info in initial_architecture:\n",
    "                if isinstance(layer_info, tuple):\n",
    "                    cls, in_dim, out_dim = layer_info\n",
    "                    layer = cls(in_dim, out_dim)\n",
    "                    # Use proper Xavier/Glorot initialization\n",
    "                    layer.weights = np.random.randn(in_dim, out_dim) * np.sqrt(2.0 / in_dim)\n",
    "                    layers.append(layer)\n",
    "                else:\n",
    "                    layers.append(layer_info())    \n",
    "            self.model = Sequential(layers) \n",
    "            \n",
    "            # Create fresh optimizer\n",
    "            self.optimizer = SGD(learning_rate=lr_params['initial_lr'])\n",
    "            \n",
    "            # Create fresh scheduler for this fold (warmup)\n",
    "            kwargs['lr_scheduler'] = WarmupLRScheduler(\n",
    "                initial_lr=lr_params['initial_lr'],\n",
    "                warmup_epochs=kwargs.get('warmup_epochs', 50)\n",
    "                )        \n",
    "        \n",
    "             # Create data loaders with consistent batch size\n",
    "            train_loader = DataLoader(dataset, indices=train_idx, \n",
    "                                batch_size=kwargs.get('batch_size', 32),\n",
    "                                shuffle=True)\n",
    "            val_loader = DataLoader(dataset, indices=val_idx,\n",
    "                              batch_size=kwargs.get('batch_size', 32))\n",
    "        \n",
    "        \n",
    "            # Train this fold\n",
    "            kwargs['show_plots'] = False  # Disable per-fold plots\n",
    "           # train_loader = DataLoader(dataset, indices=train_idx)\n",
    "           # val_loader = DataLoader(dataset, indices=val_idx)\n",
    "        \n",
    "            # Regular training but store history\n",
    "            history = self.train(train_loader, val_loader, **kwargs)\n",
    "            cv_visualizer.add_fold_history(history)\n",
    "            \n",
    "            # Add loss landscape visualization after training each fold\n",
    "            if kwargs.get('debug', False):\n",
    "                print(f\"\\nLoss landscape for fold {fold_idx + 1}\")\n",
    "                cv_visualizer.plot_loss_landscape(self.model, val_loader, self.loss_fn)\n",
    "\n",
    "        \n",
    "            # Compute fold score\n",
    "            fold_score = self.validate(val_loader)\n",
    "            fold_scores.append(fold_score)\n",
    "        \n",
    "            # Keep track of best model\n",
    "            if fold_score < best_score:\n",
    "                best_score = fold_score\n",
    "                best_model = copy.deepcopy(self.model)\n",
    "    \n",
    "        cv_visualizer.plot_k_fold_results()\n",
    "        \n",
    "        # Restore best model\n",
    "        self.model = best_model\n",
    "        \n",
    "        # Plot final loss landscape\n",
    "        if kwargs.get('debug', False):\n",
    "            print(\"\\nFinal model loss landscape\")\n",
    "            cv_visualizer.plot_loss_landscape(\n",
    "                self.model,\n",
    "                DataLoader(dataset, batch_size=kwargs.get('batch_size', 32)),\n",
    "                self.loss_fn\n",
    "            )\n",
    "    \n",
    "    \n",
    "        # Show aggregated results\n",
    "        cv_visualizer.plot_k_fold_results()\n",
    "        \n",
    "        \n",
    "    \n",
    "        mean_score = np.mean(fold_scores)\n",
    "        std_score = np.std(fold_scores)\n",
    "        print(f\"\\nCross-validation score: {mean_score:.4f} Â± {std_score:.4f}\")\n",
    "        return {\n",
    "            'mean_score': mean_score,\n",
    "            'std_score': std_score,\n",
    "            'best_model': best_model,\n",
    "            'fold_scores': fold_scores,\n",
    "            'visualizer': cv_visualizer\n",
    "        }\n",
    "        \n",
    "    def cleanup(self):\n",
    "        \"\"\"Release resources and reset state\"\"\"\n",
    "        plt.close('all')\n",
    "        self.visualizer = TrainingVisualizer()\n",
    "        \n",
    "    def __enter__(self):\n",
    "        return self\n",
    "        \n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        self.cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c2fc86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# put everything together and train the model on spiral dataset with cross-validation\n",
    "\n",
    "x_train, y_train = generate_spiral_data(1000, 5)\n",
    "x_test, y_test = generate_spiral_data(200, 5)\n",
    "train_dataset = Dataset(x_train, y_train)\n",
    "test_dataset = Dataset(x_test, y_test)\n",
    "\n",
    "model = Sequential([\n",
    "    Linear(2,64),\n",
    "    ReLU(),\n",
    "    Linear(64, 32),\n",
    "    ReLU(),\n",
    "    Linear(32, 5),\n",
    "])\n",
    "\n",
    "loss_fn = CrossEntropySoftMax()\n",
    "optimizer = SGD(learning_rate=0.1)\n",
    "trainer = Trainer(model, loss_fn, optimizer)\n",
    "\n",
    "\n",
    "trainer.train_with_cv(\n",
    "    train_dataset, \n",
    "    cv=CrossValidator(\"k-fold\", k=5),\n",
    "    debug=False,\n",
    "    epochs=2000,\n",
    "    patience=150,\n",
    "    log_interval=100,\n",
    "    batch_size=32,\n",
    "    show_plots_logs=True,\n",
    "    lr_scheduler= WarmupLRScheduler(\n",
    "        initial_lr=0.01, \n",
    "        warmup_epochs=100,\n",
    "    ),\n",
    "    weight_decay=0.001,\n",
    "    momentum=0.9,\n",
    "    )\n",
    "\n",
    "# Create test loader and evaluate\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=True)\n",
    "test_acc = trainer.compute_accuracy(test_loader)\n",
    "print(f\"\\nFinal Test Accuracy: {test_acc:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
